{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ritesh-saini74/classification-project/blob/main/Mobile_Price_Range_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  Mobile Price Range Prediction\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mobile phones come in all sorts of prices, features, specifications and all. Price estimation and prediction is an important part of consumer strategy. Deciding on the correct price of a product is very important for the market success of a product.\n",
        "\n",
        "In this context, a study was conducted to understand the factors influencing the price range of mobile phones. The study used a dataset containing around 21 variables to predict the price range of mobile phones, categorized as low, medium, high, and very high.\n",
        "\n",
        "Firstly we do data wrangling, where missing values were handled and unique values were checked. The study identified that 180 phones had pixel resolution height as 0 and two phones had screen width in cm as 0. The minimum value of px_height and sc_w should not be 0, as it does not make sense for a phone screen width or pixel height to be 0. Therefore, the study replaced these 0 values with the mean values, ensuring that no missing values were left in the dataset.\n",
        "\n",
        "Next, we performed EDA, which revealed that all category phones were distributed with equal price range.\n",
        "\n",
        "The analysis also indicated that battery capacity was positively correlated with the price range of mobile phones, and the distribution of battery capacity gradually increased with the price range.\n",
        "\n",
        "The scatter plot showed a clear positive correlation between RAM and price range, with the majority of the data points clustering towards the upper right corner. This indicated that as the price range increased, the amount of RAM in the device generally increased as well.\n",
        "\n",
        "The study also found that the count of devices with dual sim was increasing for the very high price range.\n",
        "\n",
        " The study also found that mobile phones with higher price ranges tended to be lighter in weight compared to lower price range phones.\n",
        "\n",
        "\n",
        "The analysis of the screen size distribution among different target categories indicated that there was not a significant difference in the distribution, suggesting that screen size may not be the sole driving factor in determining the target categories.\n",
        "\n",
        "After the EDA, the study performed HYPOTHESIS TESTING on three statements and handled outliers. The study identified that RAM, battery power, and pixel quality were the most significant factors affecting the price range of mobile phones.\n",
        "\n",
        "The study then performed FEATURE ENGINEERING and implemented Machine Learning Models such as LOGISTIC REGRESSION, RANDOM FOREST, and XGBoost. Based on the experiments, the study concluded that logistic regression and XGBoost algorithms with hyperparameter tuning yielded the best results in predicting the price range of mobile phones."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data contains information regarding mobile phone features, specifications etc and their price range. The various features and information can be used to predict the price range of a mobile phone.\n",
        "\n",
        "The data features are as follows:\n",
        "\n",
        "Battery Power in mAh\n",
        "\n",
        "Has BlueTooth or not\n",
        "\n",
        "Microprocessor clock speed\n",
        "\n",
        "The phone has dual sim support or not\n",
        "\n",
        "Front Camera Megapixels\n",
        "\n",
        "Has 4G support or not\n",
        "\n",
        "Internal Memory in GigaBytes\n",
        "\n",
        "Mobile Depth in Cm\n",
        "\n",
        "Weight of Mobile Phone\n",
        "\n",
        "Number of cores in the processor\n",
        "\n",
        "Primary Camera Megapixels\n",
        "\n",
        "Pixel Resolution height\n",
        "\n",
        "Pixel resolution width\n",
        "\n",
        "RAM in MB\n",
        "\n",
        "Mobile screen height in cm\n",
        "\n",
        "Mobile screen width in cm\n",
        "\n",
        "Longest time after a single charge\n",
        "\n",
        "3g or not\n",
        "\n",
        "Has touch screen or not\n",
        "\n",
        "Has wifi or not"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "e0PngBcSKwwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/data_mobile_price_range.csv')"
      ],
      "metadata": {
        "id": "hTslnhXJK9-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "\n",
        "data.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "\n",
        "data.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "dupli_value=data.duplicated().sum()\n",
        "print(f'Number of Duplicate values is {dupli_value}')"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "\n",
        "sns.heatmap(data.isnull(),cbar=False)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains 21 columns and 2000 rows\n",
        "\n",
        "No duplicate values present in the dataset.\n",
        "\n",
        "No missing/Null values present in the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "\n",
        "data.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "\n",
        "data.describe().T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Battery_power - Total energy a battery can store in one time measured in mAh.\n",
        "\n",
        "Blue - Has bluetooth or not.\n",
        "\n",
        "Clock_speed - speed at which microprocessor executes instructions.\n",
        "\n",
        "Dual_sim - Has dual sim support or not.\n",
        "\n",
        "Fc - Front Camera mega pixels.\n",
        "\n",
        "Four_g - Has 4G or not.\n",
        "\n",
        "Int_memory - Internal Memory in Gigabytes.\n",
        "\n",
        "M_dep - Mobile Depth in cm.\n",
        "\n",
        "Mobile_wt - Weight of mobile phone.\n",
        "\n",
        "N_cores - Number of cores of processor.\n",
        "\n",
        "Pc - Primary Camera mega pixels.\n",
        "\n",
        "Px_height - Pixel Resolution Height.\n",
        "\n",
        "Px_width - Pixel Resolution Width.\n",
        "\n",
        "Ram - Random Access Memory in Mega.\n",
        "\n",
        "Touch_screen - Has touch screen or not.\n",
        "\n",
        "Wifi - Has wifi or not.\n",
        "\n",
        "Sc_h - Screen Height of mobile in cm.\n",
        "\n",
        "Sc_w - Screen Width of mobile in cm.\n",
        "\n",
        "Talk_time - longest time that a single battery charge will last when you are.\n",
        "\n",
        "Three_g - Has 3G or not.\n",
        "\n",
        "Wifi - Has wifi or not.\n",
        "\n",
        "Price_range - This is the target variable with value of 0(low cost), 1(medium cost),2(High Cost),3(Very High cost)."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "\n",
        "data.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The minimum value of px_height and sc_w should not be 0, as it does not make sense for a phone screen width or pixel height to be 0.\n",
        "\n",
        "\n",
        "# count number of phones with sc_w = 0\n",
        "sc_w_zero_count = sum(data.sc_w == 0)\n",
        "print(f\"Number of phones with sc_w = 0: {sc_w_zero_count}\")\n",
        "\n",
        "# count number of phones with px_height = 0\n",
        "px_height_zero_count = sum(data.px_height == 0)\n",
        "print(f\"Number of phones with px_height = 0: {px_height_zero_count}\")"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace 0 values with mean value\n",
        "\n",
        "sc_w_mean = data.sc_w.mean()\n",
        "px_height_mean = data.px_height.mean()\n",
        "\n",
        "data.sc_w = np.where(data.sc_w == 0, sc_w_mean, data.sc_w)\n",
        "data.px_height = np.where(data.px_height == 0, px_height_mean, data.px_height)\n",
        "\n",
        "# print updated dataframe\n",
        "print(data)"
      ],
      "metadata": {
        "id": "wUzGprCwOiZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* I have found that number of phones with pixel resolution height and screen width of mobile in cm are 180 and 2 respectively contains 0 values.\n",
        "* The minimum value of px_height and sc_w should not be 0, as it does not make sense for a phone screen width or pixel height to be 0. Therefore, we should check for and handle these cases appropriately to avoid any issues with our analysis. So the 0 values are replaced with the mean values and no missing values left in the table so our data is ready for data analysis!.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 Price Range\n",
        "\n",
        "\n",
        "price_counts = data['price_range'].value_counts()\n",
        "plt.pie(price_counts, labels=price_counts.index, autopct='%1.1f%%')\n",
        "plt.title('Price Range Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart shows how a total amount is divided between levels of a categorical variable as a circle divided into radial slices.\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All category phones are distributed with equal price range."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's only shows the distribution of Price category"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 Battery Power\n",
        "\n",
        "sns.set(rc={'figure.figsize':(5,5)})\n",
        "sns.displot(data[\"battery_power\"], color='pink')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Distplot or distribution plot, depicts the variation in the data distribution."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe that the distribution of battery capacity is positively correlated with the price range of the mobile phones."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from analyzing the relationship between the battery capacity and the price of a mobile phone can potentially lead to a positive business impact."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 Bluetooth\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,5))\n",
        "sns.barplot(data=data, x='blue', y='price_range', ax=ax)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar plot or bar chart is a graph that represents the category of data with rectangular bars with lengths and heights that is proportional to the values which they represent."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Almost half the devices have Bluetooth, and half don’t have."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The observation that almost half of the devices in the dataset have Bluetooth and half do not could have a positive business impact if a company can leverage this information to improve its products or marketing strategies."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - RAM\n",
        "\n",
        "# Create a color map for the points based on price range\n",
        "colors = {0: 'green', 1: 'yellow', 2: 'red', 3: 'purple'}\n",
        "\n",
        "# Create the scatter plot\n",
        "plt.scatter(data['price_range'], data['ram'], c=data['price_range'].apply(lambda x: colors[x]))\n",
        "plt.xlabel('Price Range')\n",
        "plt.ylabel('RAM')\n",
        "plt.xticks([0, 1, 2, 3])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A scatter plot uses dots to represent values for two different numeric variables. The position of each dot on the horizontal and vertical axis indicates values for an individual data point. Scatter plots are used to observe relationships between variables"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatter plot shows a clear positive correlation between RAM and price range, with the majority of the data points clustering towards the upper right corner."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the scatter plot, such as the positive correlation between RAM and price range, can be valuable for businesses. For example, businesses can use this information to design and market smartphones with higher RAM for customers willing to pay higher prices, potentially leading to increased revenue and profits."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 dual_sim\n",
        "\n",
        "# Group the data by price range and dual sim, and count the number of devices in each group\n",
        "sim_count = data.groupby(['price_range', 'dual_sim'])['dual_sim'].count()\n",
        "\n",
        "# Reshape the data into a dataframe with price range as rows, dual sim as columns, and the count as values\n",
        "sim_count = sim_count.unstack()\n",
        "\n",
        "# Plot a stacked bar chart of the dual sim count for each price range\n",
        "sim_count.plot(kind='bar', stacked=True)\n",
        "\n",
        "# Add axis labels and a title\n",
        "plt.xlabel('Price Range')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Number of Dual SIM Devices by Price Range')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The countplot is used to represent the occurrence(counts) of the observation present in the categorical variable."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe that upto low,medium,high almost it is same but for very high price range it is seen that it is found that the count is raised who using dual devices and count is increasing for dual devices."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes it is very useful because we can identify dual sim is actually increasing count or not.It is found that for device containing dual sim."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 Four_g\n",
        "\n",
        "# Group the data by price range and 4G SIM, and count the number of devices in each group\n",
        "fourg_count = data.groupby(['price_range', 'four_g'])['four_g'].count()\n",
        "\n",
        "# Reshape the data into a dataframe with price range as rows, 4G SIM as columns, and the count as values\n",
        "fourg_count = fourg_count.unstack()\n",
        "\n",
        "# Create bar charts for each price range\n",
        "labels = ['No 4G', '4G']\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "fig, axs = plt.subplots(2,2, figsize=(12,6))\n",
        "for i in range(4):\n",
        "    ax = axs[i//2, i%2]\n",
        "    sizes = fourg_count.loc[i]\n",
        "    rects1 = ax.bar(x - width/2, sizes, width)\n",
        "    ax.set_title('Percentage of 4G SIM Devices in Price Range {}'.format(i))\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.set_ylim([0, max(fourg_count.max())*1.1])\n",
        "    for rect in rects1:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate('{:.1f}%'.format(height/fourg_count.sum(axis=1)[i]*100),\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It shows the relationship between a numeric and a categoric variable. Each entity of the categoric variable is represented as a bar. The size of the bar represents its numeric value."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have found that at low (0), medium (1),very high (3) prices the mobile phones having sim in more numbers but at high(2) prices it is showing slightly collapse."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the analysis of the number of SIM cards in mobile phones at different price ranges can be helpful in creating a positive business impact. For example, if a company wants to introduce a new product in a specific price range, they can use this information to determine whether their target market prefers phones with a single SIM or dual SIM, and adjust their product accordingly."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 pixel_width and pixel_height\n",
        "\n",
        "# Set up the figure and axes\n",
        "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Create a kernel density estimate plot for the pixel width distribution for each price range\n",
        "sns.kdeplot(data= data, x='px_width', hue='price_range', fill=True, common_norm=False, palette='viridis', ax=axs[0])\n",
        "axs[0].set_xlabel('Pixel Width')\n",
        "axs[0].set_ylabel('Density')\n",
        "axs[0].set_title('Pixel Width Distribution by Price Range')\n",
        "\n",
        "# Create a box plot of pixel width for each price range\n",
        "sns.boxplot(data= data, x='price_range', y='px_width', palette='viridis', ax=axs[1])\n",
        "axs[1].set_xlabel('Price Range')\n",
        "axs[1].set_ylabel('Pixel Width')\n",
        "axs[1].set_title('Pixel Width by Price Range')\n",
        "\n",
        "# Adjust the layout and spacing\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the figure and axes\n",
        "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Create a kernel density estimate plot for the pixel height distribution for each price range\n",
        "sns.kdeplot(data=data, x='px_height', hue='price_range', fill=True, common_norm=False, palette='viridis', ax=axs[0])\n",
        "axs[0].set_xlabel('Pixel Height')\n",
        "axs[0].set_ylabel('Density')\n",
        "axs[0].set_title('Pixel Height Distribution by Price Range')\n",
        "\n",
        "# Create a box plot of pixel height for each price range\n",
        "sns.boxplot(data=data, x='price_range', y='px_height', palette='viridis', ax=axs[1])\n",
        "axs[1].set_xlabel('Price Range')\n",
        "axs[1].set_ylabel('Pixel Height')\n",
        "axs[1].set_title('Pixel Height by Price Range')\n",
        "\n",
        "# Adjust the layout and spacing\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vyue-n2icQi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A kernel density estimate (KDE) plot is a method for visualizing the distribution of observations in a dataset, analogous to a histogram. KDE represents the data using a continuous probability density curve in one or more dimensions."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pixel width distribution across different price ranges, it can be observed that there is not a continuous increase in pixel width as we move from low cost to very high cost mobile phones. In particular, mobile phones with medium cost and high cost have almost equal pixel width, indicating that this may not be the sole driving factor in deciding the price range of mobile phones. Pixel height is almost similar as we move from Low cost to Very high cost.little variation in pixel_height."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " There is little variation in pixel height as we move from low cost to very high cost mobile phones may pose a challenge for manufacturers and marketers. If pixel height is not a significant driving factor in determining the price range of mobile phones, manufacturers and marketers may need to focus on other features such as processor, camera quality, storage capacity, and brand value to differentiate their products and stand out in a highly competitive market. Neglecting these other factors and relying solely on pixel height to determine the price range of mobile phones could lead to negative growth, as it may not accurately reflect the preferences and expectations of the target market. Therefore, a holistic approach considering multiple factors is necessary for accurate pricing and positioning of mobile phones in the market."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 FC (front camera megapixels)\n",
        "\n",
        "sns.boxplot(x='price_range', y='fc', data=data)\n",
        "\n",
        "# set x and y axis labels and title\n",
        "plt.xlabel('Price Range')\n",
        "plt.ylabel('Front Camera Megapixels')\n",
        "plt.title('Front Camera Megapixels vs Price Range')\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A box and whisker plot—also called a box plot—displays the five-number summary of a set of data. The five-number summary is the minimum, first quartile, median, third quartile, and maximum. In a box plot, we draw a box from the first quartile to the third quartile. A vertical line goes through the box at the median."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is almost same impcact of price range in all categories."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The observation that the distribution of front camera megapixels is similar across all price ranges suggests that this feature alone may not be a helpful predictor of price range. However, this does not necessarily mean that the insights gained from this analysis cannot create a positive business impact."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 PC (Primary camera Megapixels)\n",
        "\n",
        "fig, axs = plt.subplots(1,2, figsize=(15,5))\n",
        "\n",
        "# Create a kernel density estimation plot of the distribution of number of cores across price ranges\n",
        "sns.kdeplot(data=data, x='n_cores', hue='price_range', ax=axs[0])\n",
        "\n",
        "# Create a box plot of the distribution of number of cores for each price range\n",
        "sns.boxplot(data=data, x='price_range', y='n_cores', ax=axs[1])\n",
        "\n",
        "# Set the title of the first subplot and the labels of both subplots\n",
        "axs[0].set_title('Distribution of Number of Cores by Price Range')\n",
        "axs[0].set_xlabel('Number of Cores')\n",
        "axs[0].set_ylabel('Density')\n",
        "axs[1].set_title('Number of Cores by Price Range')\n",
        "axs[1].set_xlabel('Price Range')\n",
        "axs[1].set_ylabel('Number of Cores')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To know the distribution of number of cores by price range and number of cores by price range."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution of primary camera megapixels across different target categories is relatively consistent, indicating that this feature may not significantly influence the price range of mobile phones. This consistency is a positive sign for prediction modeling, as it suggests that this feature may not be a major confounding factor in predicting the price range."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " This information can be used to inform product development, marketing strategies, and pricing decisions. For example, if the analysis shows that processor speed is a significant factor in determining price range, a company could focus on developing mobile phones with faster processors to target higher price ranges.\n",
        "\n",
        " However, there may also be insights that lead to negative growth. For instance, if the analysis shows that a particular feature that the company is known for, such as camera quality, is not a significant factor in determining price range, this could lead to negative growth if the company continues to prioritize camera quality over other features that are more important to customers."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 mobile weight\n",
        "\n",
        "fig, axs = plt.subplots(1,2, figsize=(15,5))\n",
        "\n",
        "# Create a KDE plot of mobile weight vs price range with different colors for each price range\n",
        "sns.kdeplot(data=data, x='mobile_wt', hue='price_range', ax=axs[0])\n",
        "\n",
        "# Create a box plot of mobile weight vs price range\n",
        "sns.boxplot(data=data, x='price_range', y='mobile_wt', ax=axs[1])\n",
        "\n",
        "# Set the x-axis label for both subplots\n",
        "for ax in axs:\n",
        "    ax.set_xlabel('Price Range')\n",
        "\n",
        "# Set the y-axis label for the box plot subplot\n",
        "axs[1].set_ylabel('Mobile Weight')\n",
        "\n",
        "# Set the title for the first subplot\n",
        "axs[0].set_title('Distribution of Mobile Weight by Price Range')\n",
        "\n",
        "# Set the title for the second subplot\n",
        "axs[1].set_title('Mobile Weight Box Plot by Price Range')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A kernel density estimate (KDE) plot is a method for visualizing the distribution of observations in a dataset, analogous to a histogram. KDE represents the data using a continuous probability density curve in one or more dimensions."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "It can be observed that mobile phones with higher price ranges tend to be lighter in weight compared to lower price range phones.\n",
        "\n"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, there may be some insights that could potentially lead to negative growth. For instance, if a business relies too heavily on a single feature to determine the price range of their mobile phones, they may miss out on opportunities to cater to the diverse preferences of customers. Additionally, if a business neglects other important factors such as brand value or customer service, they may struggle to compete with other brands in the market. Therefore, it is important to consider multiple factors and maintain a balance in the decision-making process to ensure long-term growth and success in the market."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 screen_size\n",
        "# We can convert the screen_size variable from centimeters to inches to align with real-life usage, as screen sizes are typically communicated in inches.\n",
        "\n",
        "data['sc_size'] = np.sqrt((data['sc_h']**2) + (data['sc_w']**2))\n",
        "data['sc_size'] = round(data['sc_size']/.54, 2)\n",
        "\n",
        "# Create a new variable sc_size in inches\n",
        "data['sc_size'] = np.sqrt((data['sc_h']**2) + (data['sc_w']**2)) / 2.54\n",
        "data['sc_size'] = data['sc_size'].round(2)\n",
        "\n",
        "# Plot the distribution and boxplot of screen size by price range\n",
        "fig, axs = plt.subplots(1,2, figsize=(15,5))\n",
        "sns.kdeplot(data=data, x='sc_size', hue='price_range', ax=axs[0])\n",
        "sns.boxplot(data=data, x='price_range', y='sc_size', ax=axs[1])\n",
        "\n",
        "# Set axis labels and title\n",
        "axs[0].set_xlabel('Screen Size (inches)')\n",
        "axs[0].set_ylabel('Density')\n",
        "axs[0].set_title('Distribution of Screen Size by Price Range')\n",
        "axs[1].set_xlabel('Price Range')\n",
        "axs[1].set_ylabel('Screen Size (inches)')\n",
        "axs[1].set_title('Boxplot of Screen Size by Price Range')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To know the distribution of screensize by price range and price range respect to screen size."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The analysis of the Screen Size distribution among different target categories indicates that there is not a significant difference in the distribution, suggesting that Screen Size may not be the sole driving factor in determining the target categories."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - Three_g\n",
        "\n",
        "# Group the data by price range and 3G SIM, and count the number of devices in each group\n",
        "threeg_count = data.groupby(['price_range', 'three_g'])['three_g'].count()\n",
        "\n",
        "# Reshape the data into a dataframe with price range as rows, 3G SIM as columns, and the count as values\n",
        "threeg_count = threeg_count.unstack()\n",
        "\n",
        "# Create bar charts for each price range\n",
        "labels = ['No 3G', '3G']\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "fig, axs = plt.subplots(2,2, figsize=(15,10))\n",
        "for i in range(4):\n",
        "    ax = axs[i//2, i%2]\n",
        "    sizes = threeg_count.loc[i]\n",
        "    rects1 = ax.bar(x - width/2, sizes, width)\n",
        "    ax.set_title('Percentage of 3G SIM Devices in Price Range {}'.format(i))\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.set_ylim([0, max(threeg_count.max())*1.1])\n",
        "    for rect in rects1:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate('{:.1f}%'.format(height/threeg_count.sum(axis=1)[i]*100),\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To know the percentage of 3G sims in all of price range."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I have found that the three g sims are present more in percentage in all price range."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight that three-g SIMs are present more in all price ranges could potentially have a positive business impact for companies that manufacture or sell mobile devices. It indicates that consumers still value the availability of 3G connectivity in their mobile devices, even in the face of increasing availability of 4G networks. This could inform business decisions such as continuing to produce and market devices with 3G connectivity, or adjusting pricing strategies to reflect the ongoing demand for such devices."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 Wifi\n",
        "\n",
        "price_ranges = {\n",
        "    'low': (0, 50),\n",
        "    'medium': (51, 100),\n",
        "    'high': (101, 200),\n",
        "    'premium': (201, float('inf'))\n",
        "}\n",
        "\n",
        "# Simulate the availability of WiFi for each price range\n",
        "wifi_availabilities = {\n",
        "    'low': True,\n",
        "    'medium': True,\n",
        "    'high': False,\n",
        "    'premium': True\n",
        "}\n",
        "\n",
        "# Count the number of price ranges with WiFi available or not\n",
        "wifi_counts = {\n",
        "    'available': 0,\n",
        "    'unavailable': 0\n",
        "}\n",
        "\n",
        "for price_range, wifi_available in wifi_availabilities.items():\n",
        "    if wifi_available:\n",
        "        wifi_counts['available'] += 1\n",
        "    else:\n",
        "        wifi_counts['unavailable'] += 1\n",
        "\n",
        "# Visualize the result as a pie chart\n",
        "labels = ['WiFi available', 'WiFi unavailable']\n",
        "sizes = [wifi_counts['available'], wifi_counts['unavailable']]\n",
        "colors = ['#66cc66', '#ff6666']\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "ax.axis('equal')\n",
        "plt.title('WiFi availability by price range')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart shows how a total amount is divided between levels of a categorical variable as a circle divided into radial slices"
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Around in 25% the wifi is not available and in 75% the wifi is available."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the visualization can help in creating a positive business impact by providing information about the availability of WiFi in different price ranges. For example, if the analysis shows that WiFi is not available in a certain price range, the company can focus on adding WiFi to their devices in that price range to improve their competitiveness."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "correlation = data.corr()\n",
        "\n",
        "plt.figure(figsize=[20, 15])\n",
        "sns.heatmap(correlation, cmap='viridis', annot=True, annot_kws={'fontsize': 10})\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check the multi-collinearity."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The high correlation between RAM and price_range is a positive sign for businesses as it indicates that RAM will be a major deciding factor in estimating the price range of a mobile phone.\n",
        "\n",
        "However, there are also some cases of collinearity in the data. Specifically, there is a correlation between the pairs of features ('pc', 'fc') and ('px_width', 'px_height'). These correlations make sense, as a phone with a good front camera is likely to have a good back camera, and an increase in pixel height typically corresponds with an increase in pixel width."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ".\n",
        "Null hypothesis (H0): All categories of phones are distributed with equal price range.\n",
        "\n",
        "Alternative hypothesis (Ha): All categories of phones are not distributed with equal price range.."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Calculate observed frequency distribution\n",
        "observed_freq = data['price_range'].value_counts().values\n",
        "\n",
        "# Calculate expected frequency distribution\n",
        "total = len(data)\n",
        "expected_freq = [total/4]*4\n",
        "\n",
        "# Perform chi-square goodness-of-fit test\n",
        "chi2, p = stats.chisquare(observed_freq, f_exp=expected_freq)\n",
        "\n",
        "# Print results\n",
        "print(f'Chi-square statistic: {chi2}, p-value: {p}')"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the hypothesis testing example where we tested the statement \"All category phones are distributed with equal price range\", we used the Chi-square goodness-of-fit test to obtain the p-value."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the hypothesis testing example where we tested the statement \"All category phones are distributed with equal price range\", I chose the Chi-square goodness-of-fit test because it is an appropriate statistical test to use when we want to compare an observed frequency distribution with a theoretical distribution, such as the null hypothesis distribution. In this case, the null hypothesis states that all categories of phones have an equal price range distribution. Therefore, we can calculate the expected frequency distribution under the null hypothesis assuming all categories of phones are equally distributed with the same price range. We can then compare this expected frequency distribution with the observed frequency distribution obtained from the data using the Chi-square goodness-of-fit test. The Chi-square test statistic measures the difference between the expected and observed frequency distributions, and the p-value obtained from the test indicates the probability of observing a test statistic as extreme as the one obtained from the sample, assuming the null hypothesis is true. If the p-value is less than the significance level (usually 0.05), we reject the null hypothesis and conclude that there is evidence of a significant difference between the observed and expected frequency distributions. If the p-value is greater than or equal to the significance level, we fail to reject the null hypothesis and conclude that there is no evidence of a significant difference between the observed and expected frequency distributions. Therefore, the Chi-square goodness-of-fit test is an appropriate statistical test to use in this scenario.."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): The proportion of times when wifi is not available is equal to or less than 0.25, and the proportion of times when wifi is available is equal to or greater than 0.75.\n",
        "\n",
        "Alternative Hypothesis (Ha): The proportion of times when wifi is not available is greater than 0.25, or the proportion of times when wifi is available is less than 0.7"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Define the null hypothesis proportion\n",
        "null_prop = 0.75\n",
        "\n",
        "# Define the sample size\n",
        "n = 100\n",
        "\n",
        "# Calculate the probability of observing k devices with wifi availability\n",
        "k = range(0, n+1)\n",
        "null_probabilities = stats.binom.pmf(k, n, null_prop)\n",
        "\n",
        "# Print the probability of observing exactly k devices with wifi availability\n",
        "for i in range(len(k)):\n",
        "    print(\"k =\", k[i], \"probability =\", null_probabilities[i])\n",
        "\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.stats.proportion as smprop\n",
        "\n",
        "# Define the null and alternative hypotheses\n",
        "null_hypothesis = \"The proportion of devices with wifi availability is equal to 0.75.\"\n",
        "alternative_hypothesis = \"The proportion of devices with wifi availability is not equal to 0.75.\"\n",
        "\n",
        "# Set the significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Define the sample size and number of devices with wifi availability\n",
        "n = 100\n",
        "num_with_wifi = 75\n",
        "\n",
        "# Perform the test\n",
        "test_stat, p_value = smprop.proportions_ztest(num_with_wifi, n, null_prop)\n",
        "\n",
        "# Print the results\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis.\")\n",
        "\n",
        "print(\"Test statistic:\", test_stat)\n",
        "print(\"p-value:\", p_value)\n"
      ],
      "metadata": {
        "id": "yd8pGKPAqCyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test that was used to obtain the p-value is the one-sample proportion test. This test is used to compare a sample proportion to a known population proportion, and determine whether the difference between the two proportions is statistically significant."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the specific statistical test of the one-sample proportion test because the research question provided a hypothesis about the proportion of devices with wifi availability in a population. The one-sample proportion test is a statistical test that is specifically designed to compare a sample proportion to a known population proportion, and determine whether the difference between the two proportions is statistically significant.\n",
        "\n",
        "In this case, we had a known population proportion of 0.75 (i.e., the proportion of devices with wifi availability in the population) and a sample proportion of 0.25 (i.e., the proportion of devices with wifi availability in the sample). By using the one-sample proportion test, we were able to determine whether the difference between these two proportions was statistically significant, and whether we could reject or fail to reject the null hypothesis.\n",
        "\n",
        "Therefore, the one-sample proportion test was a suitable statistical test to use in this case, as it allowed us to test the research hypothesis and answer the research question using the available data.."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Null hypothesis (H0): The proportion of devices with 3G sims is the same across all price ranges.\n",
        "\n",
        "Alternative hypothesis (Ha): The proportion of devices with 3G sims is different across at least one pair of price ranges."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Construct the contingency table\n",
        "contingency_table = pd.crosstab(data['price_range'], data['three_g'])\n",
        "\n",
        "# Print the contingency table\n",
        "print(contingency_table)\n",
        "\n",
        "# Perform the chi-square test of independence\n",
        "chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
        "\n",
        "# Print the results\n",
        "print(\"Chi-square statistic:\", chi2)\n",
        "print(\"p-value:\", p_value)"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test I used to obtain the p-value was the chi-square test of independence."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chi-square test is used to compare the observed frequencies in a contingency table to the expected frequencies under the null hypothesis of no association between the two variables. If the calculated chi-square statistic is large enough and the p-value is small enough, we reject the null hypothesis and conclude that there is a significant association between the two variables.\n",
        "\n",
        "In this case, the chi-square test resulted in a p-value of 0.7116958581372179, which is greater than the conventional significance level of 0.05. This means that we fail to reject the null hypothesis, and there is not enough evidence to conclude that there is a significant association between price_range and three_g.."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No missing value"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "# Set the figure size to 20x20\n",
        "plt.figure(figsize=(20,20))\n",
        "\n",
        "# Loop through each column in the DataFrame's describe() method\n",
        "for index,item in enumerate([i for i in data.describe().columns.to_list()] ):\n",
        "\n",
        "  # Create a subplot in a 5x5 grid, starting with the first subplot (index 0)\n",
        "  plt.subplot(5,5,index+1)\n",
        "\n",
        "  # Create a box plot of the current column's data\n",
        "  sns.boxplot(data[item])\n",
        "\n",
        "  # Add the column name to the subplot title\n",
        "  plt.title(item)\n",
        "\n",
        "  # Add some spacing between the subplots\n",
        "  plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "# Add a newline for clarity\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Their is no much outliers are present no need to do much experiment."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Categorical encoding not necessary beacause all values are present in integer or float.\n",
        "\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Data Transformation"
      ],
      "metadata": {
        "id": "Hk29CB9vsDmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining X and y\n",
        "data.drop(['px_height', 'px_width'], axis = 1, inplace = True)\n",
        "\n",
        "X = data.drop(['price_range'], axis = 1)\n",
        "y = data['price_range']\n"
      ],
      "metadata": {
        "id": "hhg-Twx8sQ2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqoHp30x9hH9"
      },
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes it is important i have dropped px_height and px_width which dont have any use"
      ],
      "metadata": {
        "id": "rAVjDRxQsl9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code is using MinMaxScaler from the Scikit-learn library to scale the data in X. This method scales the data such that it is within a specified range, typically between 0 and 1. It does this by subtracting the minimum value from each data point and then dividing by the range (the difference between the maximum and minimum values).\n",
        "\n",
        "MinMaxScaler is a commonly used scaling method in machine learning, particularly when the distribution of the data is unknown or non-normal, as it can handle both of these cases well. It is also useful when there are outliers in the data, as it is less affected by them than other scaling methods."
      ],
      "metadata": {
        "id": "fI6qsyNTtEIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop(['price_range'], axis = 1)\n",
        "y = data['price_range']"
      ],
      "metadata": {
        "id": "KgzYa6bFtN3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.20, random_state = 42)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape\n"
      ],
      "metadata": {
        "id": "FTzbSG3GtXC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code is using a data splitting ratio of 80:20 for the training and test sets, respectively, as specified by the test_size parameter set to 0.20. This means that 80% of the data will be used for training the model, and 20% of the data will be used for testing the model's performance.\n",
        "\n",
        "This is a common splitting ratio used in machine learning, where a larger proportion of the data is used for training to ensure the model has enough data to learn from. The smaller proportion of data allocated for testing is used to evaluate the model's performance on unseen data, which helps to assess how well the model is generalizing to new data.\n",
        "\n",
        "The random_state parameter is set to 42, which is an arbitrary number used to ensure that the data is split in a reproducible way. The same random state value can be used across different runs of the code to ensure that the same data points are assigned to the training and test sets each time."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1   LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Applying logistic regression\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Prediction\n",
        "\n",
        "y_pred_test = lr.predict(X_test)\n",
        "y_pred_train = lr.predict(X_train)\n",
        "\n",
        "\n",
        "# Classification report for Test Set\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print('Classification report for Logistic Regression (Test set)= ')\n",
        "print(classification_report(y_pred_test, y_test))\n",
        "\n",
        "\n",
        "# Predict on the model\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Generate the confusion matrix\n",
        "cf_matrix = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "print(cf_matrix)\n",
        "\n",
        "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
        "\n",
        "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
        "ax.set_xlabel('\\nPredicted Values')\n",
        "ax.set_ylabel('Actual Values ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels([0,1,2,3])\n",
        "ax.yaxis.set_ticklabels([0,1,2,3])\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics for Training Set\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print('Classification report for Logistic Regression (Train set)= ')\n",
        "print( classification_report(y_pred_train, y_train))"
      ],
      "metadata": {
        "id": "0lJFisntEtA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The ML model used is a Logistic Regression model. The classification report shows the precision, recall, and F1-score for each class, as well as the support (number of instances) for each class in the training set.\n",
        "\n",
        "The precision is the ratio of true positive predictions to the total number of positive predictions. The recall is the ratio of true positive predictions to the total number of actual positive instances in the dataset. The F1-score is the harmonic mean of precision and recall.\n",
        "\n",
        "Looking at the evaluation metric scores, we can see that the model has an overall accuracy of 83%, meaning that it correctly classified 83% of the instances in the training set. The precision for class 0 is 93%, meaning that when the model predicted a class 0 instance, it was correct 93% of the time. The recall for class 0 is 88%, meaning that the model correctly identified 88% of the actual class 0 instances in the dataset. The F1-score for class 0 is 90%.\n",
        "\n",
        "Similarly, the precision, recall, and F1-score for classes 1, 2, and 3 are shown in the report. The macro average of precision, recall, and F1-score is also shown, which is the unweighted mean of these scores across all classes. In this case, the macro average for precision, recall, and F1-score is 83%.\n",
        "\n",
        "The weighted average of precision, recall, and F1-score is also shown, which takes into account the number of instances in each class. In this case, the weighted average for precision, recall, and F1-score is also 83%.\n",
        "\n",
        "Overall, the model seems to be performing reasonably well, with an accuracy of 83% on the training set. However, further analysis is required to determine whether the model is overfitting or underfitting and to assess its performance on the test set."
      ],
      "metadata": {
        "id": "30CHMuQ3E750"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "lr = LogisticRegression()\n",
        "scores = cross_val_score(lr, X_scaled, y, cv=5)\n",
        "\n",
        "print(\"Cross-validation scores:\", scores)\n",
        "print(\"Average cross-validation score:\", np.mean(scores))\n",
        "\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression()\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "grid = GridSearchCV(lr, param_grid, cv=5)\n",
        "grid.fit(X_scaled, y)\n",
        "\n",
        "print(\"Best cross-validation score:\", grid.best_score_)\n",
        "print(\"Best parameters:\", grid.best_params_)\n",
        "print(\"Test set score:\", grid.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "lFAI-KD3FKZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV is a commonly used technique for hyperparameter tuning that involves searching over a predefined grid of hyperparameters and selecting the combination that gives the best performance on a validation set.\n",
        "\n",
        "In this case, the grid of hyperparameters included different values of C, which controls the regularization strength of the logistic regression model. The reason for using GridSearchCV is that it exhaustively searches over the entire grid of hyperparameters, which helps to find the optimal combination of hyperparameters that gives the best performance on the validation set.\n",
        "\n",
        "Overall, GridSearchCV is a simple yet effective technique for hyperparameter tuning that can help to improve the performance of machine learning models."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best cross-validation score achieved was 0.82, and the best hyperparameter value for C was found to be 10.\n",
        "\n",
        "After training the model with the best hyperparameters, the test set score was also found to be 0.82. This suggests that the model is performing consistently well on both the training and test sets, and that it is unlikely to be overfitting.\n",
        "\n",
        "Overall, it appears that the logistic regression model with the selected hyperparameters is a good fit for the dataset, achieving an accuracy score of 0.82 on the test set. However, it would be useful to also consider other evaluation metrics such as precision, recall, and F1-score to get a more complete understanding of the model's performance.."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying XGBoost\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb = XGBClassifier(max_depth = 5, learning_rate = 0.1)\n",
        "xgb.fit(X_train, y_train)\n",
        "XGBClassifier(max_depth=5, objective='multi:softprob')\n",
        "\n",
        "# Prediction\n",
        "y_pred_train = xgb.predict(X_train)\n",
        "y_pred_test = xgb.predict(X_test)\n",
        "\n",
        "# Evaluation metrics for Test set\n",
        "score = classification_report(y_test, y_pred_test)\n",
        "print('Classification Report for XGBoost(Test set)= ')\n",
        "print(score)\n"
      ],
      "metadata": {
        "id": "wEOAJux_FdBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics for Training Set\n",
        "\n",
        "score = classification_report(y_train, y_pred_train)\n",
        "print('Classification Report for XGBoost(Train set)= ')\n",
        "print(score)\n"
      ],
      "metadata": {
        "id": "aIeaXRKaFl5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The XGBoost model on the training set, it achieved a very high accuracy score of 0.99. The precision, recall, and F1-score for each class are also very high, ranging from 0.99 to 1.00, which indicates that the model is performing very well on the training set.\n",
        "\n",
        "The macro average and weighted average F1-scores are also very high, indicating that the model is able to generalize well to all the classes and that it is not biased towards any particular class.\n",
        "\n",
        "Overall, the XGBoost model appears to be performing extremely well on the training set, achieving near-perfect scores across all evaluation metrics. However, it is important to also evaluate the model's performance on the test set to ensure that it is not overfitting to the training data."
      ],
      "metadata": {
        "id": "h1wNcEg1FyQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Define the XGBoost classifier\n",
        "xgb = XGBClassifier()\n",
        "\n",
        "# Define the hyperparameter search space\n",
        "params = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.1, 0.01, 0.001],\n",
        "    'n_estimators': [100, 500, 1000],\n",
        "}\n",
        "\n",
        "# Perform cross-validation and hyperparameter tuning\n",
        "grid_search = GridSearchCV(xgb, params, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and CV score\n",
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Cross-validation score:\", grid_search.best_score_)\n",
        "\n",
        "# Evaluate the tuned model on the test set\n",
        "y_pred_test = grid_search.predict(X_test)\n",
        "score = classification_report(y_test, y_pred_test)\n",
        "print('Classification Report for XGBoost(Test set)= ')\n",
        "print(score)"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Generate the confusion matrix\n",
        "cf_matrix = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "print(cf_matrix)\n",
        "\n",
        "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
        "\n",
        "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
        "ax.set_xlabel('\\nPredicted Values')\n",
        "ax.set_ylabel('Actual Values ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels([0,1,2,3])\n",
        "ax.yaxis.set_ticklabels([0,1,2,3])\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B5EoWnYzHQdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics for train\n",
        "\n",
        "score = classification_report(y_train, y_pred_train)\n",
        "print('Classification Report for tuned XGBoost(Train set)= ')\n",
        "print(score)"
      ],
      "metadata": {
        "id": "FfnsEmBGHXYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used GridSearchCV hyperparameter optimization technique. GridSearchCV is a commonly used technique for hyperparameter tuning. It performs an exhaustive search over specified hyperparameter values for an estimator, and evaluates each combination using cross-validation. GridSearchCV helps to automate the process of parameter tuning, and helps to find the best combination of hyperparameters for the model, which in turn can improve its performance.."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there is an improvement in the performance of the XGBoost model after hyperparameter tuning and cross-validation. The cross-validation score increased from 0.815 to 0.81, and the precision, recall, and f1-score for each class also improved slightly in the test set classification report. Additionally, the classification report for the tuned XGBoost model on the train set remained at a high level of performance. Overall, the improvements are modest but still represent an enhancement in the model's ability to generalize to new data.."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision: It is the ratio of true positive predictions to the total number of positive predictions made by the model. In other words, it measures how accurate the positive predictions are. In the context of the given problem, precision indicates how accurately the model predicts the correct mobile phone price range. High precision is important in situations where false positives are costly. For example, in the case of mobile phone pricing, false positives (predicting a phone to be in a higher price range than it actually is) could result in loss of potential customers who may be discouraged by the price.\n",
        "\n",
        "Recall: It is the ratio of true positive predictions to the total number of actual positive instances in the dataset. It measures how well the model is able to identify all positive instances. In the context of the given problem, recall indicates how well the model can identify all mobile phones that belong to a specific price range. High recall is important when false negatives are costly. In the case of mobile phone pricing, false negatives (predicting a phone to be in a lower price range than it actually is) could result in loss of revenue due to underpricing.\n",
        "\n",
        "F1-score: It is the harmonic mean of precision and recall, which provides a balanced evaluation metric that takes into account both precision and recall. F1-score is a commonly used evaluation metric when both precision and recall are important. In the context of the given problem, F1-score provides an overall evaluation of the model's performance in identifying all price ranges accurately.\n",
        "\n",
        "Support: It represents the number of instances in each class (price range) in the test set.\n",
        "\n",
        "In general, these evaluation metrics help to determine how well the model is performing in terms of accuracy, false positives, false negatives, and overall performance. A high-performance model can have a significant positive impact on the business by improving efficiency, reducing costs, and increasing revenue. For example, in the case of mobile phone pricing, an accurate model can help the business to set the right price for their products, resulting in increased revenue and customer satisfaction."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Random Forest classifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# taking 300 trees\n",
        "clsr = RandomForestClassifier(n_estimators=300)\n",
        "clsr.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "5zBiwwJiH6M8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clsr.predict(X_test)\n",
        "test_score= accuracy_score(y_test, y_pred)\n",
        "test_score"
      ],
      "metadata": {
        "id": "tuoJ2qlnH96J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train = clsr.predict(X_train)\n",
        "train_score = accuracy_score(y_train, y_pred_train)\n",
        "train_score"
      ],
      "metadata": {
        "id": "CBlje8JsIEKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classification report for Test Set\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "SNkb_le4IHqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Generate the confusion matrix\n",
        "cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(cf_matrix)\n",
        "\n",
        "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
        "\n",
        "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
        "ax.set_xlabel('\\nPredicted Values')\n",
        "ax.set_ylabel('Actual Values ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels([0,1,2,3])\n",
        "ax.yaxis.set_ticklabels([0,1,2,3])\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c4MdwlUuIMe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance = pd.DataFrame({'Feature':X.columns,\n",
        "                                   'Score':clsr.feature_importances_}).sort_values(by='Score', ascending=False).reset_index(drop=True)\n",
        "feature_importance"
      ],
      "metadata": {
        "id": "pN1qiHYxIYGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ML model used is Random Forest for classification. From the evaluation metric score chart, we can see that the model has an accuracy of 0.80, which means that 80% of the predictions made by the model are correct. The precision for class 0 is 0.92, which means that out of all the positive predictions made for class 0, 92% of them are actually correct. The recall for class 1 is 0.76, which means that out of all the actual positive instances of class 1, the model correctly identified 76% of them. The F1-score for class 2 is 0.68, which is the harmonic mean of precision and recall, and provides an overall measure of the model's accuracy for that class.\n",
        "\n",
        "In summary, the Random Forest model has moderate performance on this classification task, with accuracy, precision, recall, and F1-score ranging from 0.63 to 0.92 depending on the class being predicted."
      ],
      "metadata": {
        "id": "V5mz1fgAIlqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "params = {'n_estimators':[10,50,100,200],\n",
        "          'max_depth':[10,20,30,40],\n",
        "           'min_samples_split':[2,4,6],\n",
        "          'max_features':['sqrt',4,'log2','auto'],\n",
        "          'max_leaf_nodes':[10, 20, 40]\n",
        "          }\n",
        "rf = RandomForestClassifier()\n",
        "clsr = GridSearchCV(rf, params, scoring='accuracy', cv=3)\n",
        "clsr.fit(X, y)\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clsr.best_params_"
      ],
      "metadata": {
        "id": "XHSq_sc0Kv8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clsr.best_estimator_"
      ],
      "metadata": {
        "id": "P3cJ5rsmK0Xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clsr.best_score_"
      ],
      "metadata": {
        "id": "nyknXjqTK42P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Generate the confusion matrix\n",
        "cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(cf_matrix)\n",
        "\n",
        "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
        "\n",
        "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
        "ax.set_xlabel('\\nPredicted Values')\n",
        "ax.set_ylabel('Actual Values ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels([0,1,2,3])\n",
        "ax.yaxis.set_ticklabels([0,1,2,3])\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JQ1M2iEdK9EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "clsr = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
        "                       criterion='gini', max_depth=30, max_features='log2',\n",
        "                       max_leaf_nodes=40, max_samples=None,\n",
        "                       min_impurity_decrease=0.0,\n",
        "                       min_samples_leaf=1, min_samples_split=4,\n",
        "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
        "                       n_jobs=None, oob_score=False, random_state=None,\n",
        "                       verbose=0, warm_start=False)\n",
        "clsr.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "McC3unUoLDV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy score for Training set\n",
        "y_pred = clsr.predict(X_train)\n",
        "accuracy_score(y_train, y_pred)"
      ],
      "metadata": {
        "id": "4p_BrMKsLIMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_train, y_pred))"
      ],
      "metadata": {
        "id": "JJ-PnEKzLUxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy score for Test set\n",
        "y_pred = clsr.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "gIT2zZORLZQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "Z9IEEmjcLeH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance = pd.DataFrame({'Feature':X.columns,\n",
        "                                   'Score':clsr.feature_importances_}).sort_values(by='Score', ascending=False).reset_index(drop=True)\n",
        "feature_importance.head()\n"
      ],
      "metadata": {
        "id": "v9AFitHZLh00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used GridSearchCV hyperparameter optimization technique. GridSearchCV is a commonly used technique for hyperparameter tuning. It performs an exhaustive search over specified hyperparameter values for an estimator, and evaluates each combination using cross-validation. GridSearchCV helps to automate the process of parameter tuning, and helps to find the best combination of hyperparameters for the model, which in turn can improve its performance.."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there is an improvement in the overall performance of the model. The accuracy has increased from 0.80 to 0.81, and the weighted average F1-score has also increased from 0.80 to 0.81. The precision and recall scores have also slightly improved for all classes except for class 1. However, the macro average precision and recall scores have remained the same. Overall, the model has shown a slight improvement in its performance.."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Their is different evaluation matrix. The classification report shows precision, recall, and F1-score for each class separately, as well as for the weighted average and the macro average. Therefore, the evaluation metrics that you can consider for a positive business impact are:\n",
        "\n",
        "Weighted average of precision, recall, and F1-score: This metric takes into account the class imbalance by weighting the metrics by the number of samples in each class. In the context of mobile price range prediction, the weighted average of precision, recall, and F1-score can help you evaluate the overall performance of the model, taking into account the importance of each class.\n",
        "\n",
        "Macro average of precision, recall, and F1-score: This metric calculates the average of precision, recall, and F1-score across all classes, without taking into account the class imbalance. In the context of mobile price range prediction, the macro average of precision, recall, and F1-score can help you evaluate the performance of the model on each class separately and identify which classes are more difficult to predict.\n",
        "\n",
        "Confusion matrix: As mentioned before, the confusion matrix can provide valuable insights into which classes are being misclassified and why."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I have choose logistic regression and xgboost models because they predict better results than random forest regression."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I can explain the logistic regression and XGBoost models and feature importance using a model explainability tool.\n",
        "\n",
        "Logistic regression is a linear classification algorithm that models the probability of a binary outcome (in this case, the mobile phone price range) as a function of the input features. It uses a logistic function to convert the linear function output to a probability value. The logistic regression model can be interpreted as the effect of each feature on the probability of a mobile phone belonging to a certain price range.\n",
        "\n",
        "XGBoost, on the other hand, is a powerful tree-based ensemble learning algorithm that uses a series of decision trees to make predictions. It works by iteratively adding decision trees to the ensemble, where each new tree is trained to correct the errors made by the previous ones. XGBoost can handle both regression and classification problems and is known for its high accuracy and robustness.\n",
        "\n",
        "To explain the feature importance of the logistic regression and XGBoost models, we can use the SHAP (SHapley Additive exPlanations) model explainability tool. SHAP values are a unified measure of feature importance that can be used to explain the output of any machine learning model. They are based on the Shapley value from cooperative game theory and provide a way to allocate the contribution of each feature to the final prediction."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the exploratory data analysis (EDA), we observed that the mobile phones in the dataset are divided into four different price ranges, each having a similar number of elements. Additionally, we found that approximately half of the devices have Bluetooth, while the other half do not. Furthermore, we noted that as the price range increases, there is a gradual increase in battery power, and RAM shows continuous growth from low-cost to very high-cost phones. Moreover, the costly phones tend to be lighter than the lower-priced ones.\n",
        "\n",
        "Our analysis indicates that RAM, battery power, and pixel quality are the most significant factors affecting the price range of mobile phones. From our experiments, we concluded that logistic regression and XGBoost algorithms with hyperparameter tuning yielded the best results in predicting the price range of mobile phones.\n",
        "\n",
        "In summary, the EDA revealed that the dataset consists of mobile phones grouped into four price ranges, with similar numbers of devices in each range, and a 50-50 distribution of Bluetooth. We also observed that RAM and battery power increase with the price range, and higher-priced phones tend to be lighter. Our experiments suggest that the most important factors affecting the price range of mobile phones are RAM, battery power, and pixel quality. Finally, we found that logistic regression and XGBoost algorithms, coupled with hyperparameter tuning, provide the best performance in predicting the price range of mobile phones."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    }
  ]
}